# Sentra CLI config (key=value)
# runtime_preference: llama-inproc | local-binary | mock
runtime_preference=llama-inproc
sessions_dir=.sentra/sessions
state_file=.sentra/state.conf
models_file=models.tsv
default_model_id=mistral7b_v03_q4km
system_prompt=You are Sentra, an offline local-first terminal assistant.
max_tokens=256
context_window_tokens=2048
profile=balanced
llama_n_threads=0
llama_n_threads_batch=0
llama_n_batch=512
llama_offload_kqv=false
llama_op_offload=false

# For local-binary runtime, use placeholders:
# - {model_path}
# - {prompt}
# - {max_tokens}
# Example with llama.cpp's llama-cli:
# local_command_template=llama-cli -m {model_path} -n {max_tokens} --no-display-prompt -p {prompt}
local_command_template=llama-cli -m {model_path} -n {max_tokens} --no-display-prompt -p {prompt}
