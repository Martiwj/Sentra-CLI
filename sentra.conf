# Sentra CLI config (key=value)
# runtime_preference: mock | local-binary
runtime_preference=mock
sessions_dir=.sentra/sessions
system_prompt=You are Sentra, an offline local-first terminal assistant.
max_tokens=256

# If using local-binary runtime, set a command template with {prompt}.
# Example for llama.cpp's llama-cli:
# local_command_template=llama-cli -m ./models/model.gguf -n 256 --no-display-prompt -p {prompt}
local_command_template=
